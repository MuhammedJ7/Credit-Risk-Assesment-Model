{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ce3a0-88b9-4902-8fc8-f2aafe41033f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install imblearn\n",
    "!pip install tensorflow\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40414863-f18b-4ddc-8f45-8ce7d2169c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560211cd-9a17-4d3a-a5ee-f6c29ec20c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Standard Preprocessing\n",
    "\"\"\"\n",
    "CreditRisk =pd.read_csv('credit_risk_dataset.csv')\n",
    "\n",
    "# Filter age and employment length\n",
    "crData = CreditRisk[(CreditRisk['person_age'] <= 70) & (CreditRisk['person_emp_length'] < 47)].copy()\n",
    "\n",
    "# Fill missing values and drop 'loan_grade' column\n",
    "crData.loc[:, 'loan_int_rate'] = crData['loan_int_rate'].fillna(crData['loan_int_rate'].median())\n",
    "crDataCopy = crData.drop('loan_grade', axis=1)\n",
    "\n",
    "display(crDataCopy.shape)\n",
    "crDataCopy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f4b02-28ae-4fa1-b281-512d271bbe94",
   "metadata": {},
   "source": [
    "## Categorical Features Treament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3d35a-d0bf-47c1-8385-f9872938b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crdataCat_tmnt = crDataCopy.copy()\n",
    "person_home_ownership = pd.get_dummies(crdataCat_tmnt['person_home_ownership'], drop_first=True).astype(int)\n",
    "loan_intent = pd.get_dummies(crdataCat_tmnt['loan_intent'], drop_first=True).astype(int)\n",
    "\n",
    "# Convert default_on_file to binary\n",
    "crdataCat_tmnt['cb_person_default_on_file_binary'] = np.where(crdataCat_tmnt['cb_person_default_on_file'] == 'Y', 1, 0)\n",
    "\n",
    "# Data scaling\n",
    "numeric_columns = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n",
    "                   'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n",
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(crdataCat_tmnt[numeric_columns]), \n",
    "                         columns=numeric_columns, index=crdataCat_tmnt.index)\n",
    "\n",
    "# Combine scaled and categorical data\n",
    "scaled_data_combined = pd.concat([scaled_df, person_home_ownership, loan_intent], axis=1)\n",
    "scaled_data_combined['cb_person_default_on_file'] = crdataCat_tmnt['cb_person_default_on_file_binary']\n",
    "scaled_data_combined['loan_status'] = crdataCat_tmnt['loan_status']\n",
    "\n",
    "# Separate features and target\n",
    "target = scaled_data_combined['loan_status']\n",
    "features = scaled_data_combined.drop('loan_status', axis=1)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873264b4-7290-4708-beaf-aadb8143a6e0",
   "metadata": {},
   "source": [
    "## SMOTE - Synthetic Minority Over-Sampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f5661-478a-4470-90ca-9236735c64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote= SMOTE()\n",
    "balanced_features, balanced_target = smote.fit_resample(features, target)\n",
    "print (\"Shape of Balanced target:\", balanced_target.shape)\n",
    "print(\"Class distribution:\")\n",
    "print(pd.Series(balanced_target).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1627f4a-b58f-4331-b5b0-c50d5db9282a",
   "metadata": {},
   "source": [
    "## Traditional Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9014555-68f7-402d-931d-0d674ea17d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split (balanced_features, balanced_target, test_size=0.20, random_state=42)# Split the data\n",
    "\n",
    "\"\"\"RF Model\"\"\"\n",
    "rf= RandomForestClassifier ()\n",
    "rf.fit (x_train, y_train)\n",
    "print(rf.score(x_train, y_train))\n",
    "rf_prediction = rf.predict(x_test)\n",
    "features_imp_rf = pd.DataFrame ({'features' :balanced_features.columns,'rf_imp' : rf.feature_importances_})\n",
    "\n",
    "\n",
    "\"\"\" XG Boost\"\"\"\n",
    "xgb_model = XGBClassifier(tree_method = 'exact')\n",
    "#model.fit(x,y.values.ravel())\n",
    "xgb_model.fit(x_train,y_train.values.ravel())\n",
    "print (xgb_model.score(x_train,y_train.values.ravel()))\n",
    "xgb_prediction = xgb_model.predict(x_test)\n",
    "features_imp_xgb = pd.DataFrame ({'features' :balanced_features.columns,'xgb_imp' : xgb_model.feature_importances_})\n",
    "\n",
    "\n",
    "\"\"\"RNN Model\"\"\"\n",
    "# Reshape input data for RNN\n",
    "timesteps = 1  # Adjust this if want to consider temporal aspects\n",
    "features_per_timestep = x_train.shape[1]\n",
    "x_train_rnn = x_train.values.reshape((x_train.shape[0], timesteps, features_per_timestep))\n",
    "x_test_rnn = x_test.values.reshape((x_test.shape[0], timesteps, features_per_timestep))\n",
    "\n",
    "def create_rnn_model(input_dim, timesteps):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(32, return_sequences=True), input_shape=(timesteps, input_dim)),\n",
    "        Bidirectional(LSTM(16)),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_model = create_rnn_model(features_per_timestep, timesteps)\n",
    "history = rnn_model.fit(x_train_rnn, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "rnn_prediction = (rnn_model.predict(x_test_rnn) > 0.5).astype(int)\n",
    "#features_imp_rnn = pd.DataFrame ({'features' :balanced_features.columns,'rnn_imp' : rnn_model.feature_importances_})\n",
    "\n",
    "def create_dnn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(8, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "input_dim = x_train.shape[1]\n",
    "dnn_model = create_dnn_model(input_dim)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = dnn_model.fit(x_train, y_train,validation_split=0.2,epochs=50, batch_size=32,callbacks=[early_stopping],verbose=0)\n",
    "dnn_predictions = (dnn_model.predict(x_test) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15feda-8eb5-4c31-9543-cc282746a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_imp=pd.concat([features_imp_rf,features_imp_xgb],axis=1)\n",
    "features_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9e228-1b70-494a-a8a5-3f0639e10860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model Comparison\"\"\"\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1_score': f1_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "models = {\n",
    "    'Random Forest': rf_prediction,\n",
    "    'XGBoost': xgb_prediction,\n",
    "    'RNN': rnn_prediction.flatten(),\n",
    "    'DNN' : dnn_predictions.flatten()\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame({name: get_metrics(y_test, pred) for name, pred in models.items()}).T\n",
    "\n",
    "print('Model comparison:')\n",
    "print(comparison)\n",
    "\n",
    "# If you need confusion matrices:\n",
    "#confusion_matrices = {name: confusion_matrix(y_test, pred) for name, pred in models.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22b4c3-60a5-4d92-9ec2-b857bd653d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
